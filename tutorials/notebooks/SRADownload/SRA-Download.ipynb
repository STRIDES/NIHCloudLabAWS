{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273a243f",
   "metadata": {},
   "source": [
    "# Download sequence data from the NCBI Sequence Read Archive (SRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e6d5e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "DNA sequence data are typically deposited into the NCBI Sequence Read Archive, and can be accessed through the SRA website, or via command line tools called SRA Toolkit. Individual sequence entries are assigned an Accession ID, which can be used to find and download a particular file. For example, if you go to the [SRA database](https://www.ncbi.nlm.nih.gov/sra) in a browser window, and search for `SRX15695630`, you should see an entry for _C. elegans_. Here we are going to use tools from the SRA Toolkit to download a few fastq files, which can then be copied over to a cloud bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5446e9",
   "metadata": {},
   "source": [
    "### 1) Download SRA data using SRA Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07f0ee",
   "metadata": {},
   "source": [
    "First, install dependencies, including mamba (you could also use conda). At the time of writing, the version of SRA tools available with the Anaconda distribution was v.2.11.0. If you want to install the latest version, download and install from [here](https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit). If you do the direct install, you will also need to configure interactively following [this guide](https://github.com/ncbi/sra-tools/wiki/05.-Toolkit-Configuration), you can do that by opening a terminal and running the commands there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1471da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\n",
    "!bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/mambaforge/bin/mamba install -c bioconda -c conda-forge sra-tools==2.11.0 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0077e",
   "metadata": {},
   "source": [
    "Test that your install works and that fasterq-dump is available in your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasterq-dump -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/ec2-user/SageMaker/NIHCloudLabAWS/tutorials/notebooks/SRADownload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb3d23",
   "metadata": {},
   "source": [
    "Set up your directory structure for the raw fastq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5686a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data data/fasterqdump/raw_fastq data/ipyrad/raw_fastq data/prefetch_fastqdump/raw_fastq data/prefetch_fasterqdump/raw_fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c720c",
   "metadata": {},
   "source": [
    "### Create Accession list and figure out how much space you need on your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46688093",
   "metadata": {},
   "source": [
    "Here we will download nine fastq files of SARS-CoV-2. You will need a list of accession IDs to download, which you can find these from papers or by searching SRA directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffee47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!for i in {1..9}; do echo \"SRR1408688$i\"; done > list_of_accesionIDS.txt\n",
    "!cat list_of_accesionIDS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74d7a0",
   "metadata": {},
   "source": [
    "Let's get the info for one of those accession numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdb-dump --info SRR14086881"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17f682",
   "metadata": {},
   "source": [
    "`Path` shows us the location. In this case the data are on GCP, but some files are on AWS, some on-prem, and others in multiple places.\n",
    "`Size` shows us the size of the compressed file in bites. You should plan to have about 10x that size available on your system, so to get a rough estimate of the space you need, just look at a few files, convert to MB/GB, then multiply by the number of accessions. In this case, are are converting the .sra file to forward and reverse files equal to ~4MB each. Obviously this is more important with large files and/or more files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf966b",
   "metadata": {},
   "source": [
    "Check the amount of space on your VM, if it is not enough, then stop the machine, resize it, and come back to here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56f6ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test stand alone fasterq dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48079a",
   "metadata": {},
   "source": [
    "Fasterq-dump is the replacement for the legacy fastq-dump tool (see below). You can read [this guide](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump) to see the full details on this tool. You can also run `fasterq-dump -h` to see most of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac67737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/fasterqdump/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82588ebc",
   "metadata": {},
   "source": [
    "Fasterq dump doesn't run in batch mode, so one way to run a command on multiple samples is by using a for loop. There are many options you can explore, but here we are running -O for outdir, -e for the number of threads, -m for memory (4GB), and --location for the location we want to retrieve the file from. Depending on the type of cloud storage, it may be faster to select `NCBI` for the location. You may consider running a few tests with one or two of your accession numbers before downloading a whole batch. The default number of threads = 6, so adjust -e based on your machine size. For large files, you may also benefit from a machine type with more memory and/or threads. You may need to stop this VM, resize it, then restart and come back. Note that there are a bunch of ways to split your fastq files (defined [here](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump)) but the default of `split 3` will split into forward, reverse, and unpaired reads.\n",
    "\n",
    "This will take about 4 minutes, the reason it is slow is that these accessions are stored on GCP, finding data stored on AWS will work much faster. Remember, to find the location of an accession, type `vdb-dump --info <Accession>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!for x in `cat ../list_of_accesionIDS.txt`; do fasterq-dump -f -O raw_fastq -e 8 -m 4G --location NCBI $x ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fff948",
   "metadata": {},
   "source": [
    "### Test Prefetch with legacy fastq dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../prefetch_fastqdump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c927f28",
   "metadata": {},
   "source": [
    "First run the prefetch tool. It will pull the compressed .sra files (ETL) files, which then need to be converted. You can also run prefetch with an explicit list of accessions, or feed in a text file with the `--option-file` command. Here we are running it with `-v` verbose. Play with the --location, switching between AWS, GCP, and NCBI, and see where it pulls the data from and you can also compare the times between locations. Also, these accessions are only stored on GCP. What happens if you put AWS as the location? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c755e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!prefetch --option-file ../list_of_accesionIDS.txt -O raw_fastq -f yes --location AWS -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b5536",
   "metadata": {},
   "source": [
    "Now we will use the legacy fastq dump tool to convert to fastq files. This is an older tool that uses single threads, so it will take a few minutes to convert the .sra files to fastq. See the full list of options by typing `fastq-dump -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!for x in `cat ../list_of_accesionIDS.txt`; do fastq-dump -O raw_fastq --split-3 $x; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a5ea7",
   "metadata": {},
   "source": [
    "### Test Prefetch plus fasterq-dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847187a9",
   "metadata": {},
   "source": [
    "Now are were going to do the same thing, but convert the ETL files using fasterq-dump. For this to work, you need to either give the path to the prefetch directories in your text file, or make sure you cd into the raw_fastq dir so that it can find those directories with the .sra files. In this case, --location GCP is a lot faster than NCBI, but feel free to run your own tests with different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4137b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../prefetch_fasterqdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!prefetch --option-file ../list_of_accesionIDS.txt -O raw_fastq -f yes --location GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8fe2dd",
   "metadata": {},
   "source": [
    "Here we won't specify a location since we are just converting the prefetch records. But notice that it goes much faster than when we first ran fasterq-dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!for x in `cat ../../list_of_accesionIDS.txt`; do fasterq-dump -f -O raw_fastq -e 8 -m 4G $x; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 mb s3://cloud-lab-tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b8802",
   "metadata": {},
   "source": [
    "### Copy data to a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66700cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp raw_fastq/ s3://cloud-lab-tutorials/raw_fastq --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://cloud-lab-tutorials/raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bed51",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The easiest single command for downloading SRA files is to use fasterq dump, which accessess the ETL files and converts to Fastq/BAM all in one command. In our example, with 8 threads, this took about 4 minutes to download the data from GCP. If however, we first use prefetch to download the ETL files, then use fasterq dump to convert the files, the whole process takes about 20 seconds. Thus, the best solution is to run two commands, first prefetch, then fasterq dump. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cbfd9",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Make sure you shut down this VM, or delete it if you don't plan to use if further."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
