{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5b1446",
   "metadata": {},
   "source": [
    "# Snakemake on AWS ParallelCluster \n",
    "\n",
    "**Skill Level: Beginner**\n",
    "\n",
    "## Overview\n",
    "\n",
    "### AWS ParallelCluster \n",
    "AWS ParallelCluster is a fully managed tool that simplifies the creation, management, and deployment of High-Performance Computing (HPC) clusters on the AWS cloud. It automates the setup and running of HPC clusters. Users can define their cluster configurations, including instance types, storage options, install scripts, and advanced networking settings like VPC, subnets, and security groups.\n",
    "\n",
    "AWS ParallelCluster integrates with  HPC schedulers like Slurm, offering robust job scheduling and resource management. \n",
    "\n",
    "### Snakemake and the `pcluster-Slurm` plugin\n",
    "\n",
    "Snakemake is a workflow manager that simplifies the process of creating and executing complex data analysis pipelines. It uses a Python-based language to define workflows and automate the execution of tasks. \n",
    "\n",
    "Snakemake workflows can be deployed seamlessly on AWS ParallelCluster using the `pcluster-Slurm` plugin. This plugin enables the use of Slurm via AWS ParallelCluster as an executor for Snakemake workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10304f5e",
   "metadata": {},
   "source": [
    "## Learning Objectives \n",
    "\n",
    "We hope that by the end of this tutorial, you will\n",
    "1. Understand the concepts of AWS ParallelCluster for High-Performance Computing.\n",
    "2. Know how to set up and configure an AWS ParallelCluster environment.\n",
    "3. Gain a basic understanding of Snakemake as a workflow manager and learn how to define and execute workflows using Snakemake's syntax.\n",
    "4. Discover how to integrate Snakemake workflows with AWS ParallelCluster using the pcluster-Slurm plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970e82b",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Access to a VPC network through your AWS account\n",
    "* Access to the AWS ParallelCluster UI \n",
    "\n",
    "Please follow the installation instructions for the ParallelCluster UI provided here: [here](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/docs/Install_AWSParallelCluster.md). These instructions will guide you through the necessary steps to create a CloudFormation Stack through which you can access the AWS ParallelCluster UI. \n",
    "\n",
    "Additionally, we urge you to check out the documents within the `docs/` folder of the repository for more bioinformatics and Gen AI tutorials.\n",
    "\n",
    "Once you have created the Cloud Formation Stack for the PCUI, navigate to the user interface URL. It will look like this:", 
"![alt text](../../docs/images/pcui.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bcc55",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Create a Cluster \n",
    "Let's create a cluster within the ParallelCluster environment.\n",
    "\n",
    "![alt text](../../docs/images/create-cluster.png)\n",
    "\n",
    "1. In the PCUI Clusters view, choose **Create cluster** > **Step by step**.\n",
    "2. In Cluster, **Name**, enter a name for your cluster.\n",
    "3. Choose a **VPC** from the available options and choose Next. CloudLab users will have access to pre-configured VPC networks.\n",
    "4. In **Head node**, choose Add **SSM session**. This will allow you to access the head node through the **`Shell`** button. Change the instance type of your head node to **t2.xlarge**. \n",
    "5. In **Queues**, provide a name and subnet for your queue.\n",
    "6. In **Compute resources**, choose 1 for **Static nodes** and select **c5n.large** as the instance type for your compute resources. \n",
    "7. In Storage, choose Next.\n",
    "8. In Cluster configuration, review the cluster configuration YAML and choose **Dry run** to \n",
    "validate it.\n",
    "1. Choose **Create** to create your cluster, based on the validated configuration.\n",
    "2.  After a few seconds, the PCUI automatically navigates you back to Clusters, where you can\n",
    "monitor the cluster creation status and Stack events.\n",
    "1.  Choose **Details** to see cluster details, such as the version and status.\n",
    "2.  Choose **Instances** to see the list of Amazon EC2 instances and status.\n",
    "3.  Choose **Stack events** to view cluster stack events, and a AWS Management Console link to the\n",
    "CloudFormation stack that creates the cluster.\n",
    "1.  In Details, after cluster creation completes, choose **View YAML** to view or download the cluster configuration YAML file.\n",
    "\n",
    "This is the YAML file for the cluster described above: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2138e1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "Imds:\n",
    "  ImdsSupport: v2.0\n",
    "HeadNode:\n",
    "  InstanceType: t2.xlarge\n",
    "  Imds:\n",
    "    Secured: true\n",
    "  LocalStorage:\n",
    "    RootVolume:\n",
    "      VolumeType: gp3\n",
    "      Size: 50\n",
    "  Networking:\n",
    "    SubnetId: subnet-0be4c22d8137b8085\n",
    "  Iam:\n",
    "    AdditionalIamPolicies:\n",
    "      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\n",
    "  Ssh:\n",
    "    KeyName: Snakemake-cluster-key-pair\n",
    "Scheduling:\n",
    "  Scheduler: Slurm\n",
    "  SlurmQueues:\n",
    "    - Name: queue-1\n",
    "      AllocationStrategy: lowest-price\n",
    "      ComputeResources:\n",
    "        - Name: queue-1-cr-1\n",
    "          Instances:\n",
    "            - InstanceType: c5n.large\n",
    "          MaxCount: 1\n",
    "          MinCount: 1\n",
    "      ComputeSettings:\n",
    "        LocalStorage:\n",
    "          RootVolume:\n",
    "            VolumeType: gp3\n",
    "      Networking:\n",
    "        SubnetIds:\n",
    "          - subnet-0be4c22d8137b8085\n",
    "  SlurmSettings: {}\n",
    "Region: us-east-1\n",
    "Image:\n",
    "  Os: alinux2\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b66c2c",
   "metadata": {},
   "source": [
    "### Set up your head node\n",
    "\n",
    "After cluster creation completes, click on the **Shell** button to access the cluster head node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954a6c4",
   "metadata": {},
   "source": [
    "1. Switch to user and create a working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef6fb3",
   "metadata": {},
   "source": [
    "```bash \n",
    "sudo -su ssm-user \n",
    "`cd ~` \n",
    "`mkdir workdir`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73692fbe",
   "metadata": {},
   "source": [
    "3. Install conda. This is required as we will be executing Snakemake using conda. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabf387",
   "metadata": {},
   "source": [
    "```bash\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "bash ~/miniconda.sh\n",
    "~/miniconda3/bin/conda init\n",
    "source ~/.bashrc\n",
    "which conda\n",
    "conda --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c90d562",
   "metadata": {},
   "source": [
    "3. Install Snakemake and the Snakemake ParallelCluster plugin. \n",
    "\n",
    "Note: the PCluster plugin requires Snakemake > 8.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aee275",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip3 install Snakemake==8.25.5\n",
    "pip3 install Snakemake-executor-plugin-pcluster-Slurm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302b6ba",
   "metadata": {},
   "source": [
    "Alternatively, you may use conda to install Snakemake using the following command: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea31ff",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda install bioconda::Snakemake==8.25.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10a99d",
   "metadata": {},
   "source": [
    "### Submitting a \"Hello World\" job to the Slurm cluster using `sbatch`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8187f0",
   "metadata": {},
   "source": [
    "You can submit jobs to the Slurm cluster using a Slurm script and the sbatch command for submission. \n",
    "\n",
    "1. Create a Slurm script. This example runs prints \"Hello World\" in an output file, then appends the file with the name of the compute node the task ran on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad0a63",
   "metadata": {},
   "source": [
    "```bash \n",
    "#hello-world.Slurm\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=hello-world\n",
    "#SBATCH --output=hello-world.out\n",
    "#SBATCH --error=hello-world.err\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=00:01:00\n",
    "\n",
    "echo \"Hello, World!\" > ~/workdir/hello-world.out\n",
    "\n",
    "## Print the hostname of the node the job ran on\n",
    "echo \"This job ran on node: $(hostname)\" >> /home/workdir/scripts/hello-world.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907420c9",
   "metadata": {},
   "source": [
    "2. Submit the job using an `sbatch` command "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086a0f7",
   "metadata": {},
   "source": [
    "```bash\n",
    "sbatch hello-world.Slurm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062fca",
   "metadata": {},
   "source": [
    "### Snakemake workflow files\n",
    "\n",
    "When running a Snakemake workflow, it is common to organize the workflow dependencies in the following structure: \n",
    "\n",
    "```bash\n",
    "Project Folder\n",
    "│\n",
    "├── Snakefile\n",
    "│\n",
    "├── config.yml\n",
    "│\n",
    "├── environment.yml\n",
    "│\n",
    "├── data\n",
    "│   ├── file 1\n",
    "│   └── file 2...\n",
    "```\n",
    "**Snakefile:** A Snakefile is the main file used in Snakemake to define a workflow. The commands to be executed, the input and output files and the dependencies of each step are defined as rules in this file. This file must be present in the working directory; if named Snakefile, Snakemake will automatically recognize it as the workflow definition file. If named differently, you must use the -s flag to specify the file. \n",
    "\n",
    "**config.yaml:** The config.yaml file is used to store configuration parameters that can be easily accessed and utilized throughout the workflow. This file allows you to define various settings, paths, parameters, and other variables that your Snakemake rules might need.\n",
    "\n",
    "**environment.yml:** The environment.yml file defines the software environment required to run the Snakemake workflow include package names and versions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57db5db",
   "metadata": {},
   "source": [
    "### Snakefile Structure\n",
    "\n",
    "The required sections of a Snakefile include the Rule definitions and the rule all. \n",
    "\n",
    "**Rules:** \n",
    "\n",
    "* Rules are the building blocks of a Snakemake workflow. Each rule describes how to create one or more output files from input files. \n",
    "* The commands that must be run, any scripts, and environment parameters can be defined in the rule definition\n",
    "\n",
    "**Rule all:**\n",
    "\n",
    "* This rule tells Snakemake what the end products should be when the workflow is complete. It is usually the first rule that is defined. \n",
    "\n",
    "**Input and Output:** \n",
    "\n",
    "* These keywords specify the files that are inputs to and outputs from a rule. \n",
    "* The input keyword lists the files needed to run the rule, and the output keyword lists the files that will be produced by the rule. \n",
    "* The order of execution of rules are determined through the input and output fields. \n",
    "\n",
    "Example: \n",
    "\n",
    "```python\n",
    "rule rule_1:\n",
    "    input: \n",
    "        \"input_1.txt\n",
    "    output:\n",
    "        \"output_1.txt\"\n",
    "\n",
    "rule rule_2: \n",
    "    input: \n",
    "        \"output_1.txt\"\n",
    "    output: \n",
    "        \"output_2.txt\" \n",
    "```\n",
    "\n",
    "**Shell Command:** \n",
    "\n",
    "The shell keyword is used to specify the shell command that will be executed to produce the output files.\n",
    "\n",
    "**Snakefile Example** \n",
    "\n",
    "A Snakefile can look like this: \n",
    "\n",
    "```bash\n",
    "# Snakefile\n",
    "\n",
    "# Rule all: specifies the final target of the workflow\n",
    "rule all:\n",
    "    input:\n",
    "        \"output_2.txt\"\n",
    "\n",
    "# Rule 1: processes input_1.txt to produce output_1.txt\n",
    "rule rule_1:\n",
    "    input:\n",
    "        \"input_1.txt\"\n",
    "    output:\n",
    "        \"output_1.txt\"\n",
    "    shell:\n",
    "        \"cat {input} > {output}\"\n",
    "\n",
    "# Rule 2: processes output_1.txt to produce output_2.txt\n",
    "rule rule_2:\n",
    "    input:\n",
    "        \"output_1.txt\"\n",
    "    output:\n",
    "        \"output_2.txt\"\n",
    "    shell:\n",
    "        \"wc -l {input} > {output}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942bfc7",
   "metadata": {},
   "source": [
    "### Submitting a \"Hello World\" script using Snakemake, Slurm and the `pcluster-Slurm` plugin\n",
    "\n",
    "Now that we've covered the basics of writing a script in Snakemake, let's create and run our first workflow! \n",
    "\n",
    "#### Create and Run the workflow\n",
    "1. Create a Snakefile within a project directory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ed425",
   "metadata": {},
   "source": [
    "```bash \n",
    "mkdir hello-world-Snakemake\n",
    "vim Snakefile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9086db",
   "metadata": {},
   "source": [
    "Add the following content to the file: \n",
    "\n",
    "```python\n",
    "#Snakefile\n",
    "rule all:\n",
    "    input:\n",
    "        \"output.txt\"\n",
    "\n",
    "rule example_rule:\n",
    "    output:\n",
    "        \"output.txt\"\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        echo 'Hello, World!' > {output}\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "**Snakefile Breakdown** \n",
    "* This Snakemake workflow defines a rule all that looks for a file called \"output.txt\" \n",
    "* In the rule definition, the shell keyword is used to echo \"Hello, World!\" in the output file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590ade3",
   "metadata": {},
   "source": [
    "2. Execute the workflow using the Snakemake command, specifying `pcluster-Slurm` as the executor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b21d7c",
   "metadata": {},
   "source": [
    "When submitting a Snakemake workflow to the Slurm scheduler integrated in AWS ParallelCluster, you can submit the workflow by using the `--executor` flag and specifying the `pcluster-Slurm` plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f34921",
   "metadata": {},
   "source": [
    "```bash\n",
    "Snakemake --executor pcluster-Slurm \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0496feb",
   "metadata": {},
   "source": [
    "#### Commandline Command Breakdown: \n",
    "\n",
    "**Snakemake:** \n",
    "\n",
    "Invokes the Snakemake tool. This tool will look for a Snakefile in the current working directory \n",
    "\n",
    "**--executor pcluster-Slurm:** \n",
    "\n",
    "The flag enables the workflow to be executed through the Slurm cluster connected to the head node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf47ae7",
   "metadata": {},
   "source": [
    "### Submitting a bioinformatics Snakemake workflow to the Slurm cluster\n",
    "\n",
    "In this example, we will use Snakemake and the pcluster-Slurm plugin to run a Bioinformatics pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d944f",
   "metadata": {},
   "source": [
    "#### Download the input data\n",
    "\n",
    "The input data consists of raw fastq files. Use the curl command to download the data from a public NIGMS google storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "#Navigate to working directory \n",
    "mkdir ~/workdir/bioinfo-workflow\n",
    "cd ~/workdir/bioinfo-workflow\n",
    "\n",
    "#Make data directory \n",
    "mkdir ./data \n",
    "cd /data\n",
    "\n",
    "#Run curl commands \n",
    "curl -o SRR13349122_1.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349122_1.fastq\n",
    "curl -o SRR13349122_2.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349122_2.fastq\n",
    "curl -o SRR13349123_1.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349123_1.fastq\n",
    "curl -o SRR13349123_2.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349123_2.fastq\n",
    "curl -o SRR13349128_1.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349128_1.fastq\n",
    "curl -o SRR13349128_2.fastq https://storage.googleapis.com/nigms-sandbox/me-inbre-rnaseq-pipelinev2/data/raw_fastqSub/SRR13349128_2.fastq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431a966",
   "metadata": {},
   "source": [
    "#### Create an `environment.yml` file\n",
    "\n",
    "```bash\n",
    "vi environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64db158",
   "metadata": {},
   "source": [
    "```yaml\n",
    "name: bioinformatics-test\n",
    "channels:\n",
    "  - bioconda\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - bwa\n",
    "  - samtools\n",
    "  - bcftools\n",
    "  - matplotlib\n",
    "  - pandas\n",
    "  - pysam\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dfc27",
   "metadata": {},
   "source": [
    "#### Create a `config.yaml` file\n",
    "\n",
    "```bash\n",
    "vi config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bd7f5",
   "metadata": {},
   "source": [
    "```yaml\n",
    "conda_env: \"envs/environment.yml\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae0487",
   "metadata": {},
   "source": [
    "#### Snakefile Breakdown:\n",
    "\n",
    "In this section, we'll breakdown what the Snakemake workflow will accomplish by looking at teh sections of the Snakefile. \n",
    "\n",
    "**Overview:** \n",
    "The Snakefile maps a set of fastq files to a reference genome, sorts and indexes the mapped reads and finally runs variant calling on the mapped reads.\n",
    "\n",
    "**Configuration:** \n",
    "* `configfile: \"config.yaml\"` specifies the configuration file you have created\n",
    "* `SAMPLES = [\"A\", \"B\"]` defines the samples to be processed.\n",
    "\n",
    "**Workflow:**\n",
    "* **all:** Specifies the final output files required to complete the workflow.\n",
    "* Bioinformatics rules \n",
    "  * **bwa_index:** Indexes the reference genome file (data/genome.fa) for alignment.\n",
    "  * **bwa_map:** Maps the sequencing reads (data/samples/{sample}.fastq) to the indexed genome and converts the output to BAM format.\n",
    "  * **samtools_sort:** Sorts the BAM files generated from the mapping step.\n",
    "  * **samtools_index:** Indexes the sorted BAM files for faster access.\n",
    "  * **bcftools_call:** Calls genetic variants from the sorted and indexed BAM files.\n",
    "  * **plot_quals:** Generates a plot of the quality of the called variants.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f99c6",
   "metadata": {},
   "source": [
    "#### Make the Snakefile \n",
    "\n",
    "Create a Snakefile and add the contents described below to it. \n",
    "\n",
    "```bash\n",
    "vi Snakefile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e54b4",
   "metadata": {},
   "source": [
    "```python\n",
    "#Snakefile\n",
    "configfile: \"config.yaml\"\n",
    "\n",
    "SAMPLES = [\"A\", \"B\"]\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"mapped_reads/{sample}.bam\", sample=SAMPLES),\n",
    "        expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n",
    "        \"calls/all.vcf\"\n",
    "\n",
    "rule bwa_index:\n",
    "    input:\n",
    "        \"data/genome.fa\"\n",
    "    output:\n",
    "        \"data/genome.fa.bwt\"\n",
    "    conda:\n",
    "        config[\"conda_env\"]\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        bwa index {input}\n",
    "        \"\"\"\n",
    "\n",
    "rule bwa_map:\n",
    "    input:\n",
    "        genome=\"data/genome.fa\",\n",
    "        fastq=\"data/samples/{sample}.fastq\",\n",
    "        index=\"data/genome.fa.bwt\"\n",
    "    output:\n",
    "        \"mapped_reads/{sample}.bam\"\n",
    "    conda:\n",
    "        config[\"conda_env\"]\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        bwa mem {input.genome} {input.fastq} > mapped_reads/{wildcards.sample}.sam\n",
    "        samtools view -Sb mapped_reads/{wildcards.sample}.sam > {output}\n",
    "        rm mapped_reads/{wildcards.sample}.sam\n",
    "        \"\"\"\n",
    "\n",
    "rule samtools_sort:\n",
    "    input:\n",
    "        \"mapped_reads/{sample}.bam\"\n",
    "    output:\n",
    "        \"sorted_reads/{sample}.bam\"\n",
    "    conda:\n",
    "        config[\"conda_env\"]\n",
    "    shell:\n",
    "        \"samtools sort -T sorted_reads/{wildcards.sample} -O bam {input} > {output}\"\n",
    "\n",
    "rule samtools_index:\n",
    "    input:\n",
    "        \"sorted_reads/{sample}.bam\"\n",
    "    output:\n",
    "        \"sorted_reads/{sample}.bam.bai\"\n",
    "    conda:\n",
    "        config[\"conda_env\"]\n",
    "    shell:\n",
    "        \"samtools index {input}\"\n",
    "\n",
    "rule bcftools_call:\n",
    "    input:\n",
    "        fa=\"data/genome.fa\",\n",
    "        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n",
    "        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n",
    "    output:\n",
    "        \"calls/all.vcf\"\n",
    "    conda:\n",
    "        config[\"conda_env\"]\n",
    "    shell:\n",
    "        \"bcftools mpileup -f {input.fa} {input.bam} | bcftools call -mv - > {output}\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f186b6f",
   "metadata": {},
   "source": [
    "#### Execute the workflow \n",
    "\n",
    "Execute the workflow using the **Snakemake** command, specifying **pcluster-Slurm** as the executor and **conda** as the environment management system\n",
    "\n",
    "```bash\n",
    "Snakemake --executor pcluster-Slurm --use-conda -j 5\n",
    "```\n",
    "#### Commandline Command Breakdown: \n",
    "\n",
    "**--use-conda** \n",
    "\n",
    "This flag tells Snakemake to use Conda environments for managing dependencies. When this flag is used, Snakemake will look for environment.yaml files specified in the workflow rules and create Conda environments accordingly. \n",
    "\n",
    "**-j** \n",
    "\n",
    "This flag specifies the number of jobs (or threads) to run in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b04fb",
   "metadata": {},
   "source": [
    "## Pricing \n",
    "\n",
    "When using AWS ParallelCluster, you only pay for the services utilized when creating and operating clusters, including computing, storage, and CloudFormation costs. A full list of services that AWS ParallelCluster may use can be found [here](https://docs.aws.amazon.com/parallelcluster/latest/ug/aws-services-v3.html).\n",
    "\n",
    "The PCUI is built on a serverless architecture and can be used through a free tier for most instances. For more information on the costs associated with the PCUI, please refer to the documentation found [here](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-pcui-costs-v3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2213dd",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Congratulations! You have successfully learned how to set up and manage High-Performance Computing (HPC) clusters on AWS using AWS ParallelCluster and you've run a Snakemake workflow on this cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cf6fa",
   "metadata": {},
   "source": [
    "## Clean Up: \n",
    "- To stop all compute nodes select, Navigate to the PCUI and select Stop fleet.\n",
    "- To clean up, in the Clusters view, select the cluster, and choose Actions, Delete cluster\n",
    "\n",
    "## References: \n",
    "* [AWS ParallelCluster](https://docs.aws.amazon.com/parallelcluster/)\n",
    "* [Snakemake Documentation](https://Snakemake.readthedocs.io/en/stable/)\n",
    "* [Snakemake `pcluster-Slurm` plugin](https://Snakemake.github.io/Snakemake-plugin-catalog/plugins/executor/pcluster-Slurm.html)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
