{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343a9759-27d1-4363-bd42-867d36d3269f",
   "metadata": {},
   "source": [
    "# Introduction to AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8a928-1080-4ddb-afb1-e5c6243c0dfd",
   "metadata": {},
   "source": [
    "**Skill Level:** Beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77fbfa-d3af-49f9-85fd-d49279951be2",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7bbfc-e4fc-47d3-8948-d341045d1f17",
   "metadata": {},
   "source": [
    "This tutorial will walk you through the process of setting up and using AWS Batch to run bioinformatic pipelines using Nextflow. By the end, you'll have a functional AWS Batch environment and understand the basics of job submission. \n",
    "\n",
    "AWS Batch is a fully managed service that enables you to run batch computing workloads on the AWS Cloud. It's designed to handle large-scale, compute-intensive tasks efficiently without the need to manage the underlying infrastructure. Here's an overview of AWS Batch along with its key benefits\n",
    "\n",
    "- **Simplified workload management** by eliminating the need to install and manage batch computing software\n",
    "- **Cost optimization** by automatically scaling resources based on job requirements, AWS Batch helps reduce costs by ensuring you only pay for the compute resources you actually use and delete the instances once the job is done.\n",
    "- **Scalability and flexibility** by handle workloads of any scale, from a few jobs to hundreds of thousands, automatically managing the required infrastructure. It supports various job types, including single-node and multi-node parallel jobs, allowing you to run diverse workloads.\n",
    "- **No upfront commitments**, there are no upfront costs or commitments required to use AWS Batch.\n",
    "- **Monitoring and logging** by providing built-in monitoring and logging capabilities through integration with AWS CloudWatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250d1b1-a3d3-4e08-94fa-cceeff2a01cc",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208362c-87fe-4f3b-9b76-555c9f1fc8a4",
   "metadata": {},
   "source": [
    "In this tutorial you will learn:\n",
    "- How to set up AWS Batch via the console\n",
    "- Run a batch job via the console\n",
    "- Run a batch job using Nextflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73351f5c-884e-49fa-a0d9-dfbbd6dc252f",
   "metadata": {},
   "source": [
    "## Prerequisites <a id='prereq'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900e87-04c3-4c5c-9c8c-f2ba3d00eab3",
   "metadata": {},
   "source": [
    "Please ensure you have a VPC, subnets, and security group set up before running this tutorial.\n",
    "\n",
    "Role necessary for this tutorial is the **'AWSServiceRoleForBatch'** role, with the following policies:\n",
    "- AdministratorAccess\n",
    "- AmazonSageMakerFullAccess\n",
    "- AWSBatchServiceRole\n",
    "\n",
    "You must also have a **\"Instance role\"**. To create an instance role follow the instructions below:\n",
    "\n",
    "Create IAM role\n",
    "- Naviagate to IAM in the console\n",
    "- In the left navigation pane, click on \"Roles\"\n",
    "- Click the \"Create role\" button\n",
    "- Under \"Trusted entity type\", select \"AWS Service\"\n",
    "- Select \"EC2\" as the use case\n",
    "- Click \"Next\"\n",
    "\n",
    "Attach Permissions Policies\n",
    "- Search for and select the following policies:\n",
    "    - AmazonECS_FullAccess\n",
    "    - AmazonEC2ContainerRegistryFullAccess\n",
    "- Click \"Next\"\n",
    "Name and Review the Role\n",
    "- Give your role a name (e.g., \"AWSBatchInstanceRole\")\n",
    "- Review the role details\n",
    "- Click \"Create role\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb7709-b420-423c-be33-09d6e8b014a6",
   "metadata": {},
   "source": [
    "## Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663a0b4-89a1-477c-b72c-8b4633412c4c",
   "metadata": {},
   "source": [
    "If you are following this tutorial in one sitting it will cost ~$2.00. Completing the process in multiple sessions or using a method different from the tutorial may result in increased costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afade387-54c8-421b-ae17-b55c07b9a5f7",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a64f5c-de63-4edf-a207-0be67a3b0c17",
   "metadata": {},
   "source": [
    "### Creating a Compute Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df6540-10e4-4e4b-8946-4d0ec84b4cb5",
   "metadata": {},
   "source": [
    "Naviate to **'AWS Batch'** in the console. Within AWS Batch on the left side menu navigate to **'Compute environments'**, select **'Create'**. AWS Batch gives you three options for a compute environment:\n",
    "\n",
    "![batch1](../../docs/images/aws_batch_1.png)\n",
    "\n",
    "- **AWS Fargate**: Is a serverless compute engine for containers, it eliminates the need to manage underlying infrastructure. Automatically scales and manages compute resources. Ideal for running containerized applications without server management.\n",
    "\n",
    "- **Amazon EC2 (Elastic Compute Cloud)**: Provides resizable compute capacity in the cloud and offers a wide variety of instance types optimized for different use cases. Gives you full control over the underlying virtual machines. Allows customization of operating systems, network, and storage configurations. Suitable for a broad range of workloads, from web servers to high-performance computing.\n",
    "\n",
    "- **Amazon EKS (Elastic Kubernetes Service)**: Managed Kubernetes service. Simplifies the deployment, management, and scaling of containerized applications using Kubernetes. Provides a fully managed control plane for Kubernetes clusters. Ideal for organizations already using Kubernetes or requiring its advanced orchestration capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ace19-d472-45d3-9222-9125d0220cc8",
   "metadata": {},
   "source": [
    "For this tutorial we will be working with EC2, select 'EC2'. \n",
    "- Select \"Managed\" for Orchestration type. \n",
    "- Enter a name for your compute environment\n",
    "- Under service role select \"AWSServiceRoleForBatch\"\n",
    "- Under Instance role select an available instance role (if you don't already have one review the instruction [here](#prereq))\n",
    "- Click 'Next'\n",
    "\n",
    "![batch2](../../docs/images/aws_batch_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e54102-5ae3-4360-9949-0c709b0dbf24",
   "metadata": {},
   "source": [
    "Now we will configure our compute environment to use **Spot Instances** to save on costs. Spot instances are spare EC2 capacity that is available for less than the On-Demand price. We have set the fields in the image below to default. Click Next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479a5a3-fee1-431f-aa09-ad9056b1c13d",
   "metadata": {},
   "source": [
    "![batch3](../../docs/images/aws_batch_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc8134-9c18-4301-bde7-62a402e0ecba",
   "metadata": {},
   "source": [
    "For Network Configuration select your VPC, subnets, and security groups you would like to utilize. This will allow AWS Batch to create instances that can communicate with each other and have access to acceptable networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879b716-517b-469d-830d-98543a2a7432",
   "metadata": {},
   "source": [
    "![batch4](../../docs/images/aws_batch_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864d6e1-c8fa-4004-a676-f7387b5cb3d1",
   "metadata": {},
   "source": [
    "The last step is to review all the configuration made to your compute environment. Once you are satisfied click \"Create compute environment\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d122bb-014b-4dfb-9cf8-7bb4a4b4428d",
   "metadata": {},
   "source": [
    "![batch5](../../docs/images/aws_batch_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ec5a9-b33d-4235-af69-4c605addb9c1",
   "metadata": {},
   "source": [
    "### Creating a Job Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b7081-22ec-43a8-a959-1575cd4ca6d7",
   "metadata": {},
   "source": [
    "Now that we have created a compute environment lets create a **job queue**. Job queues help Batch to stay organized by holding jobs until they can be scheduled to run in a compute environment.\n",
    "\n",
    "In the AWS Batch console, go to the left side menu, click \"Job queues\" and click \"Create\".\n",
    "   - Set orchestration type to EC2\n",
    "   - Give your queue a name and set its priority. for this tutorial we have se it to '1000' to have the highest priority \n",
    "   - Associate the compute environment you created in the pervious step.\n",
    "   - Review and create the job queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde68279-0ad0-485f-90a0-2af1b10f9cbd",
   "metadata": {},
   "source": [
    "![batch6](../../docs/images/aws_batch_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14e501-4add-4885-9c41-897a2e9c352d",
   "metadata": {},
   "source": [
    "### Applying Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13d30b-bf80-47a9-b3aa-5b8ea4d90661",
   "metadata": {},
   "source": [
    "For this step we are enabling AWS Batch permissions on EC2 clusters without this our jobs will not run.\n",
    "- On the left side menu under 'Control settings' click 'Permissions'\n",
    "- Next to Container insights click 'Edit'\n",
    "- Using the toggles select with compute environment should have these permissions and click 'Save changes' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb37046-067b-4894-860f-aa8f0aebce18",
   "metadata": {},
   "source": [
    "![batch7](../../docs/images/aws_batch_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950dca20-df9a-48f1-94fc-c54bdb200eac",
   "metadata": {},
   "source": [
    "Now that you have set the permissions head back to the compute environment console and ensure that your environment is flagged as valid. \n",
    "\n",
    "![batch8](../../docs/images/aws_batch_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39965cad-400b-44f7-b115-0be889c07e9d",
   "metadata": {},
   "source": [
    "### Install Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8187099-4cbb-4424-9dbd-1f39fd75b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run if you don't have Java installed\n",
    "! sudo apt update\n",
    "! sudo apt-get install default-jdk -y\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60c1f9-4bc2-4c3e-b9f3-ec0575fd8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install nexflow, make it exceutable, and update it\n",
    "! curl https://get.nextflow.io | bash\n",
    "! chmod +x nextflow\n",
    "! ./nextflow self-update#add nextflow to your path! sudo mv $PWD/nextflow /usr/local/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe631eb-7c37-4ca3-aa3c-453be740c17c",
   "metadata": {},
   "source": [
    "### Nextflow 101\n",
    "\n",
    "Nextflow interacts with many different files to have a proper working workflow:\n",
    "\n",
    "- **Main file:** The main file is a .nf file that holds the processes and channels describing the input, output, a shell script of your commands, workflow which acts like a recipe book for nextflow, and/or conditions. For snakemake users this is equivalent to 'rules'.\n",
    "- **Process:** Contains channels and scripts that can be executed in a Linux server like bash commands.\n",
    "- **Channel:** Produces ways through which processes communicate to each other for example input and output are channels of value that point the process to where data is or should be located.\n",
    "- **Config file:** The .config file contains parameters, and multiple profiles. Each profile can contain a different executor type (e.g. LS API, conda, docker, etc.), memory or machine type, output directory, working directory and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f1700-48cb-4469-b1f7-5bd2d49255e5",
   "metadata": {},
   "source": [
    "### Run a nextflow 'Hello World' process locally\n",
    "\n",
    "We are going to first run Hello World locally using the nextflow scirpt named hello.nf.\n",
    "\n",
    "It should look like this (add this code to a file named hello.nf):\n",
    "\n",
    "```\n",
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2 \n",
    "\n",
    "params.str = 'Hello World'\n",
    "\n",
    "process sayHello {\n",
    "  input:\n",
    "  val str\n",
    "\n",
    "  output:\n",
    "  stdout\n",
    "\n",
    "  \"\"\"\n",
    "  echo $str > hello.txt\n",
    "  cat hello.txt\n",
    "  \"\"\"\n",
    "}\n",
    "workflow {\n",
    "  sayHello(params.str) | view\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c21b2-1cf1-4227-abba-cc467b5faa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run your nextflow script\n",
    "! ./nextflow run hello.nf --str 'Hello!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b7f5f-e09c-4558-86c3-dfb1a27f689a",
   "metadata": {},
   "source": [
    "### Submitting a AWS Batch Nextflow Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e725a0-92cf-4540-9855-0f1e3e363753",
   "metadata": {},
   "source": [
    "Create a bucket to store our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48b5b9-964a-40ab-8bd8-584d60a8bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'ENTER_BUCKET_NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde3877-e08a-4d04-991a-96e945c8e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 mb s3://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd7929-e34d-4449-be42-1a619dd20f59",
   "metadata": {},
   "source": [
    "Create and modify your own config file to include a 'awsbatch' profile block to tell Nextflow to submit the job to AWS Batch instead of running locally\n",
    "\n",
    "The config file allows nextflow to utilize executers like AWS Batch. In this tutorial the config files is named 'nextflow.config'. Make sure you open this file and update the <BUCKET_NAME> that are account specific.\n",
    "\n",
    "Make sure that your region is a region included in the AWS Batch!\n",
    "Specify your working directory bucket and output directory bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b83d7b-ecd3-445b-a671-810d5e796dba",
   "metadata": {},
   "source": [
    "```\n",
    "plugins {\n",
    "    id 'nf-amazon'\n",
    "}\n",
    "\n",
    "profiles {\n",
    "    awsbatch {\n",
    "        process {\n",
    "            executor = 'awsbatch'\n",
    "            queue = 'test'\n",
    "            container = 'quay.io/nf-core/ubuntu:22.04'\n",
    "            \n",
    "        }\n",
    "        workDir = 's3://<BUCKET_NAME>/nextflow_env/'\n",
    "        params.outdir = 's3://<BUCKET_NAME>/nextflow_output/'\n",
    "        \n",
    "        fusion.enabled = true\n",
    "        wave.enabled = true\n",
    "        aws.region = 'us-east-1'\n",
    "\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc9fd8-d236-40b2-a4e2-b6e0365760f5",
   "metadata": {},
   "source": [
    "**Note:** Make sure your working directory and output directory are different! AWS Batch creates temporary file in the working directory within your bucket that do take up space so once your pipeline has completed successfully feel free to delete the temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf08ed-df60-4660-b76b-6787d9a2b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./nextflow run nf-core/methylseq -profile test,awsbatch -c nextflow.config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da22576-eb3f-44bf-b8f9-0b2163afce36",
   "metadata": {},
   "source": [
    "### Optional: Listing nf-core tools with docker and viewing their commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa3a54-7806-4913-b16a-1a65eb004b85",
   "metadata": {},
   "source": [
    "Using the command below you can see all the bioinformatic pipelines that nf-core holds and their versions/lastest releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f483895-7217-467e-94ba-00ef1f19729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run nfcore/tools list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eacd47-c1ea-4cd2-9de7-a486f81695a1",
   "metadata": {},
   "source": [
    "You can view commands for methylseq (or any other specified nf-core tool) by using the [--help] flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956eccc6-689a-49c2-b4d4-d0c0137234a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./nextflow run nf-core/methylseq --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c343edc-e2b2-46f2-94e5-f7aaa82463bb",
   "metadata": {},
   "source": [
    "### Monitor Job Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8601e48-c7ec-49cd-abfc-ec77f87d650b",
   "metadata": {},
   "source": [
    "   - After submitting the job, you can monitor its progress in the \"Jobs\" section of the AWS Batch console.\n",
    "   - Check the job's status, logs, and any output as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbbbc-4a04-4ea2-9c12-63d78a0a7052",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7e275-d6da-409d-8c73-0de60e49b94b",
   "metadata": {},
   "source": [
    "You've now successfully set up an AWS Batch environment, created a job definition, and submitted a job via Nextflow. This basic workflow can be expanded to handle more complex batch processing tasks, leveraging AWS Batch's scalability and management features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968d2d1-b369-4794-a339-159ee80c2c85",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3cc31-a1bc-4cd9-880a-ee625c0f7bea",
   "metadata": {},
   "source": [
    "Delete your bucket, compute environment, any unsuccessful jobs via the console. If you are using any Jupyter notebooks though AWS please ensure that you stop or delete the notebook to avoid accruing costs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
