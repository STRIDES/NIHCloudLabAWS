{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88625911",
   "metadata": {},
   "source": [
    "# Download sequence data from the NCBI Sequence Read Archive (SRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cea78e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "DNA sequence data are typically deposited into the NCBI Sequence Read Archive, and can be accessed through the SRA website, or via a collection of command line tools called SRA Toolkit. Individual sequence entries are assigned an Accession ID, which can be used to find and download a particular file. For example, if you go to the [SRA database](https://www.ncbi.nlm.nih.gov/sra) in a browser window, and search for `SRX15695630`, you should see an entry for _C. elegans_. Alternatively, you can search the SRA metadata using Amazon Athena and generate a list of accession numbers. Here we are going to generate a list of accessions using Athena, use tools from the SRA Toolkit to download a few fastq files, then copy those fastq files to a cloud bucket. We really only scratch the surface of how to search Athena using SQL. If you want more examples, you can also try the notebooks from [this SRA GitHub repo](https://github.com/ncbi/ASHG-Workshop-2021). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d4270",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "+ Learn more about the Sequence Read Archive\n",
    "+ Learn how to download SRA data locally\n",
    "+ Learn how to interact with SRA metadata via Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19079d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "+ Make sure you have access to SageMaker, Athena and Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bcdc1",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f62f42",
   "metadata": {},
   "source": [
    "### Set up environment and install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfbc50",
   "metadata": {},
   "source": [
    "### Set up your Athena Database\n",
    "You need to set up your Athena database in the Athena console before you start this notebook. Follow our [guide](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/docs/create_athena_database.md) to walk you through it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e2c86",
   "metadata": {},
   "source": [
    "At the time of writing, the version of SRA tools available with the Anaconda distribution was v.2.11.0. If you want to install the latest version, download and install from [here](https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit). If you do the direct install, you will also need to configure interactively following [this guide](https://github.com/ncbi/sra-tools/wiki/05.-Toolkit-Configuration), you can do that by opening a terminal and running the commands there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f7f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install everything else\n",
    "! mamba install -c bioconda -c conda-forge sql-magic pyathena -y #sra-tools==2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032d702",
   "metadata": {},
   "source": [
    "Test that your install works and that fasterq-dump is available in your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e68c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! fasterq-dump -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc46609",
   "metadata": {},
   "source": [
    "### Setup Directory Structure and Create a Staging Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec72dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data data/fasterqdump/raw_fastq data/prefetch_fasterqdump/raw_fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f583e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you change this name, it needs to be globally unique\n",
    "%env BUCKET=sra-data-athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf58849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will only create the bucket if it doesn't yet exist\n",
    "# if the bucket exists you won't see any output\n",
    "! aws s3 ls s3://$BUCKET >& /dev/null || aws s3 mb s3://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a50c1",
   "metadata": {},
   "source": [
    "### Create a list of SRA accessions using Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033ef70",
   "metadata": {},
   "source": [
    "Here we use Athena to generate a list of accessions. You can also generate a manual list by searching the [SRA Database](https://www.ncbi.nlm.nih.gov/sra) and saving to a file or list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1369f",
   "metadata": {},
   "source": [
    "If you get a module not found error for either of these, rerun the mamba commands above, make sure mamba is still in your path, or just use `pip install pyathena`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyathena import connect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfda5e",
   "metadata": {},
   "source": [
    "Establish connection. List your staging bucket and the region of your bucket. Make sure your bucket is in us-east-1 to avoid egress charges when downloading from sra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir='s3://sra-data-athena/',\n",
    "               region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f2dd9",
   "metadata": {},
   "source": [
    "**When you run the query in the next cell you may get this error**:\n",
    "\n",
    "`An error occurred (AccessDeniedException) when calling the StartQueryExecution operation: User: arn:aws:sts::055102001469:assumed-role/sagemaker-notebook-instance-role/SageMaker is not authorized to perform: athena:StartQueryExecution on resource: arn:aws:athena:us-east-1:055102001469:workgroup/primary because no identity-based policy allows the athena:StartQueryExecution action`\n",
    "\n",
    "If you get this error, read our [IAM guide](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/docs/update_sagemaker_role.md) to set up the correct policy for your Sagemaker role. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830d8e3",
   "metadata": {},
   "source": [
    "Now that the permissions are all set up, let's download bacterial samples from the [NIGMS Sandbox RNAseq tutorial](https://github.com/NIGMS/RNA-Seq-Differential-Expression-Analysis). You could change the SQL query as you like, feel free to take a look at the generated df, and then play with different parameters. For more inspiration of what is possible with SQL queries, look at this [SRA tutorial](https://github.com/ncbi/ASHG-Workshop-2021/blob/main/3_Biology_Example_AWS_Demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eff316",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE organism = 'Mycobacteroides chelonae' \n",
    "limit 3;\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3511937",
   "metadata": {},
   "source": [
    "As you can see, most of what you need to know is shown in this data frame. If you wanted to just show the accession, you could replace the * for acc in the SELECT command. One other thing to think about is how large are these files, and do you have space on your VM to download them? You can figure this out by looking at the 'jattr' column, and then converting the number of bites to GB, then add that for a few samples to get a ballpark figure. If you need more space, stop the VM, go to SageMaker and [resize your disk](https://aws.amazon.com/blogs/machine-learning/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker/). Make sure you stop your notebook instance to Edit and resize it. You can see the amount of space on your disk from the command line using `! df -h .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jattr'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5dfb0",
   "metadata": {},
   "source": [
    "You can also get the same info using `vdb-dump --info <ACCESSION>`. You can also get the path for the sra compressed file in a bucket using `srapath <ACCESSION>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdb-dump --info SRR13349124 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "! srapath SRR13349124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e7a97",
   "metadata": {},
   "source": [
    "Save our accession list to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_accessionIDS.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat list_of_accessionIDS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01437b57",
   "metadata": {},
   "source": [
    "### Download FASTQ files with fasterq dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff0d5e",
   "metadata": {},
   "source": [
    "Fasterq-dump is the replacement for the legacy fastq-dump tool. You can read [this guide](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump) to see the full details on this tool. You can also run `fasterq-dump -h` to see most of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fasterqdump/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37097eb4",
   "metadata": {},
   "source": [
    "Fasterq dump doesn't run in batch mode, so one way to run a command on multiple samples is by using a for loop. There are many options you can explore, but here we are running -O for outdir, -e for the number of threads, -m for memory (4GB). The default number of threads = 6, so adjust -e based on your machine size. For large files, you may also benefit from a machine type with more memory and/or threads. You may need to stop this VM, resize it, then restart and come back. There are also a bunch of ways to split your fastq files (defined [here](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump)) but the default of `split 3` will split into forward, reverse, and unpaired reads. Depending on your machine size, expect about 5 min for these three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! for x in `cat ../list_of_accessionIDS.txt`; do fasterq-dump -f -O raw_fastq -e 8 -m 4G $x ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5acc6",
   "metadata": {},
   "source": [
    "On our VM that command took 6.5 min, although with a larger machine size it will run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd52cd",
   "metadata": {},
   "source": [
    "### Download FASTQ files with prefetch + fasterq dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15200f2",
   "metadata": {},
   "source": [
    "Using the example bacterial data, fasterq dump took about 6.5 min to download the files (ml.t3.2xlarge with 8 CPUs and 32 GB RAM). Under the hood, fasterq dump is pulling the compressed sra files from the database (in this case it should be coming from AWS) and converting them on the fly, which is slow (ish) because it has to do a lot over the network. A better method is to disaggregate these functions using prefetch to pull the compressed files, then use fasterq-dump to convert them locally, rather than over the network. For this to work, you need to either give the path to the prefetch directories in your text file, or make sure you cd into the raw_fastq dir so that it can find those directories with the .sra files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../prefetch_fasterqdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! prefetch --option-file ../list_of_accessionIDS.txt -O raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eece75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb650a",
   "metadata": {},
   "source": [
    "Now convert the prefetch records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! for x in `cat ../list_of_accessionIDS.txt`; do fasterq-dump -f -O raw_fastq -e 8 -m 4G raw_fastq/$x; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49507511",
   "metadata": {},
   "source": [
    "Comparing the two methods, we can see that fasterq-dump on its own took 6.5 min, whereas prefetch + fasterq-dump takes less than 1.5 min."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea152fd7",
   "metadata": {},
   "source": [
    "### Copy Files to a Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4eef67",
   "metadata": {},
   "source": [
    "--recursive will copy a whole directory like `-r` in bash. S3 multithreads by default so you don't have to specify threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp raw_fastq/*.fastq s3://sra-data-athena/raw_fastq --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ebc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls s3://sra-data-athena/raw_fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fda05",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "Here you learned how to generate a list of accessions using Athena and then use the SRA toolkit to download FASTQ files. We tested fasterq-dump on its own, but found that using prefetch then fasterq-dump is much faster. Finally you learned how to copy directories to S3 using the `--recursive` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4026566",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Make sure you shut down this VM, or delete it if you don't plan to use if further.\n",
    "\n",
    "You can also [delete the buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-bucket.html) if you don't want to pay for the data: `aws s3 rb s3://bucket-name --force`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
